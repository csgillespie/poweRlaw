%\VignetteEngine{knitr::knitr}
%\VignetteIndexEntry{examples}

\documentclass[a4paper,justified,openany]{tufte-handout}
\setcounter{secnumdepth}{2}
\usepackage{microtype}
\usepackage{url}  % Formatting web addresses  
\usepackage[utf8]{inputenc} %unicode support
\usepackage{color,hyperref,comment, booktabs}
\urlstyle{rm}
\usepackage{algorithmic, algorithm,amsmath}
\newcommand{\cc}{\texttt}
\newcommand{\xmin}{x_{\min}}

\usepackage{etoolbox}
\makeatletter
\preto{\@verbatim}{\topsep=-20pt \partopsep=0pt }
\makeatother

\date{Last updated: \today}  
\title{The poweRlaw package: Examples}
\author[Colin S. Gillespie]{Colin S. Gillespie}
\date{Last updated: \today}

<<echo=FALSE>>=
library(knitr)
library(poweRlaw)
options(replace.assign=FALSE,width=50)

opts_chunk$set(fig.path='knitr_figure/graphics-', 
               cache.path='knitr_cache/graphics-', 
               fig.align='center', 
               dev='pdf', fig.width=5, fig.height=5, 
               fig.show='hold', cache=FALSE, par=TRUE)
knit_hooks$set(crop=hook_pdfcrop)

knit_hooks$set(par=function(before, options, envir){
    if (before && options$fig.show!='none') {
        par(mar=c(3,3,2,1),cex.lab=.95,cex.axis=.9,
            mgp=c(2,.7,0),tcl=-.01, las=1)
}}, crop=hook_pdfcrop)

#options(width=60)
#knit_theme$set(knit_theme$get()[7])
set.seed(1)
palette(c(rgb(170,93,152, maxColorValue=255),
              rgb(103,143,57, maxColorValue=255),
              rgb(196,95,46, maxColorValue=255),
              rgb(79,134,165, maxColorValue=255),
              rgb(205,71,103, maxColorValue=255),
              rgb(203,77,202, maxColorValue=255),
              rgb(115,113,206, maxColorValue=255)))

@

\begin{document}
\maketitle
\begin{abstract}
The \verb$poweRlaw$ package provides an easy to use interface for fitting and visualising heavy tailed distributions, including power-laws. The fitting procedure follows the method detailed in Clauset \textit{et al.}\cite{clauset2009} This vignette provides examples of the fitting procedure.
\end{abstract}

<<echo=FALSE, results='hide', message=FALSE, warning=FALSE, error=FALSE>>=
if(!file.exists("blackouts.txt"))
  download.file("http://goo.gl/BsqnP", destfile="blackouts.txt")
@

\section{Discrete data: Moby Dick}

The Moby Dick dataset contains the frequency of unique words in the novel Moby Dick by Herman Melville. This data set can be downloaded from
\begin{center}
\url{http://tuvalu.santafe.edu/~aaronc/powerlaws/data.htm}
\end{center}
\noindent or loaded directly
<<>>=
library("poweRlaw")
data("moby")
@
\noindent To fit a discrete power-law to this data\sidenote{The object \texttt{moby} is a simple R vector.}, we use the \texttt{displ} constructor
<<cache=TRUE>>=
m_pl = displ$new(moby)
@
\noindent The resulting object, \cc{m\_pl}, is a \cc{displ}\sidenote{\cc{displ}: discrete power-law.} object. It also inherits the \cc{discrete\_distribution} class. After creating the \cc{displ} object, a typical first step would be to infer model parameters.\sidenote{When the \cc{displ} object is first created, the default parameter values are \cc{NULL} and $\xmin$ is set to the minimum $x$-value.} We can estimate the lower threshold, via:
<<cache=TRUE>>=
est = estimate_xmin(m_pl)
m_pl$setXmin(est)
@
\noindent For a given value $\xmin$, the scaling parameter is estimated by numerically optimising the log-likelihood. The optimiser is initialised using the analytical MLE
\[
\hat \alpha \simeq 1 + n \left[\sum_{i=1}^n \log \left(\frac{x_i}{\xmin -0.5}\right)\right]^{-1} \;.
\]
\noindent This yields a threshold estimate of $\xmin=\Sexpr{est$xmin}$ and scaling parameter $\alpha=\Sexpr{signif(est$pars, 3)}$, which matches results found in \citet{clauset2009}. Alternatively, we could perform a parameter scan for each value of $\xmin$:
<<cache=TRUE,results='hide'>>=
estimate_xmin(m_pl, pars=seq(1.5, 2.5, 0.1))
@
\noindent The parameter scan will typically be slower than using the optimiser.

To fit a discrete log-normal distribution, we follow a similar procedure, except we begin by creating a \cc{dislnorm}.\sidenote{\cc{dislnorm}: discrete log-normal object}

<<cache=TRUE,warning=FALSE>>=
m_ln = dislnorm$new(moby)
est = estimate_xmin(m_ln)
@
<<echo=FALSE, cache=TRUE>>=
est_ln = est
est_ln$pars = signif(est_ln$pars, 3)
m_ln$setXmin(est)
m_pois = dispois$new(moby)
est = estimate_xmin(m_pois)
m_pois$setXmin(est)
@

\noindent which yields a lower threshold of $\xmin=\Sexpr{est_ln$xmin}$ and parameters $(\Sexpr{est_ln$pars[1]},$ $\Sexpr{est_ln$pars[2]})$. A similar procedure is applied to fit the Poisson distribution; we create a distribution object using \cc{dispois}, then fit as before.

The data CDF and lines of best fit can be easily plotted:
<<fig.keep='none', cache=TRUE>>=
plot(m_pl)
lines(m_pl, col=2)
lines(m_ln, col=3)
lines(m_pois, col=4)
@
\begin{marginfigure}
\centering
<<echo=FALSE, cache=TRUE>>=
plot(m_pl, xlab="x", ylab="CDF", 
     panel.first=grid(col="grey80"), 
     pch=21, bg=1)

lines(m_pl, col=2, lwd=2)
lines(m_ln, col=3, lwd=2)
lines(m_pois, col=4, lwd=2)
@
\caption{Data CDF of the Moby Dick data set. The fitted power-law (green line), log-normal (red line) and poisson (blue) distributions are also given.}\label{F1a}
\end{marginfigure}
\noindent to obtain figure \ref{F1a}. It clear that the Poisson distribution is not appropriate for this data set. However, the log-normal and power-law distribution both provide reasonable fits to the data.



\subsection{Parameter uncertainty}

<<echo=FALSE>>=
data(bootstrap_moby)
bs = bootstrap_moby
@

To get a handle on the uncertainty in the parameter estimates, we use a bootstrapping procedure, via the \texttt{bootstrap} function. This procedure can be applied to any distribution object.\sidenote{For example, \mbox{bootstrap(m\_ln)}.} Furthermore, the bootstrap procedure can utilize multiple CPU cores to speed up inference.\sidenote{The output of this bootstrapping procedure can be obtained via \cc{data(bootstrap\_moby)}.}
<<eval=FALSE, tidy=FALSE>>=
## 5000 bootstraps using two cores
bs = bootstrap(m_pl, no_of_sims=5000, threads=2)
@
\noindent By default, the \cc{bootstrap} function will use the maximum likelihood estimate to estimate the parameter and check all values of $\xmin$. When possible $\xmin$ values are large, then it is recommend that the search space is reduced. For example, this function call
<<eval=FALSE>>=
bootstrap(m_pl, xmins = seq(2, 20, 2))
@
\noindent will only calculate the Kolmogorov-Smirnoff statistics at values of $\xmin$ equal to
\[
2, 4, 6, \ldots, 20\;.
\]
A similar argument exists for the parameters.\sidenote{For single parameter models, \cc{pars} should be a vector. For the log-normal distribution, \cc{pars} should be a matrix of values.}

The bootstrap function, returns \cc{bs\_xmin} object that has three components:
\begin{enumerate}
\item The goodness of fit statistic obtained from the Kolmogorov-Smirnoff test. This value should correspond to the value obtained from the \mbox{\cc{estimate\_xmin}} function.
\item A data frame containing the results for the bootstrap procedure. 
\item The average simulation time, in seconds, for a single bootstrap.
\end{enumerate}
The boostrap results can be explored in a variety way. First we can estimate the standard deviation of the parameter uncertainty, i.e.

\begin{figure}[t]
\centering
<<echo=FALSE, fig.width=12, fig.height=9>>=
plot(bs)
@
\caption{Results from the standard bootstrap procedure (for the power-law model) using the Moby Dick data set: \mbox{\texttt{bootstrap(m\_pl)}}. The top row shows the mean estimate of parameters $\xmin$ and $\alpha$. The bottom row shows the estimate of standard deviation for each parameter. The dashed-lines give approximate 95\% confidence intervals.

After 5000 iterations, the standard deviation of $\xmin$ and $\alpha$ is estimated to be 2.1 and 0.03 respectively.}\label{F1b}
\end{figure}
<<>>=
sd(bs$bootstraps[,2])
sd(bs$bootstraps[,3])
@
\noindent Alternatively, we can visualise the results using the \cc{plot} function:
<<fig.keep='none'>>=
## trim=0.1 only displays the final 90% of iterations 
plot(bs, trim=0.1)
@
\noindent to obtain figure \ref{F1b}. This top row of graphics in figure \ref{F1b} give a 95\% confidence interval for mean estimate of the parameters. The bottom row of graphics give a 95\% confidence for the standard deviation of the parameters. The parameter \texttt{trim} in the \texttt{plot} function controls the percentage of samples displayed.\sidenote[][-5\baselineskip]{When \cc{trim=0}, all iterations are displayed.} When \texttt{trim=0.1}, we only display the final 90\% of data. 
\begin{marginfigure}[-7\baselineskip]
\centering
<<echo=FALSE, fig.width=4, fig.height=8, cache=TRUE>>=
par(mfrow=c(2, 1))
hist(bs$bootstraps[,2], xlab=expression(x[min]), ylim=c(0, 1600), 
     xlim=c(0, 20), main=NULL, breaks="fd")
grid()
hist(bs$bootstraps[,3], xlab=expression(alpha), 
     ylim=c(0, 500), xlim=c(1.80, 2.05), main=NULL, breaks="fd")
grid()

@
\caption{Characterising uncertainty in parameter values. (a) $\xmin$ uncertainty (standard deviation 2) (b) $\alpha$ uncertainty (std dev. 0.03)}\label{F1c}
\end{marginfigure}

We can also construct histograms. 
<<fig.keep='none'>>=
hist(bs$bootstraps[,2])
hist(bs$bootstraps[,3]) 
@
\noindent to get figure \ref{F1c}. 

A similar bootstrap analysis can be obtained for the log-normal distribution
<<eval=FALSE>>=
bs1 = bootstrap(m_ln)
@
\noindent in this case we would obtain uncertainty estimates for both of the log-normal parameters.

<<echo=FALSE>>=
data(bootstrap_p_moby)
bs_p = bootstrap_p_moby
@
\begin{figure}[t]
\centering
<<echo=FALSE, fig.width=12, fig.height=9, cache=TRUE>>=
plot(bs_p)
@
\caption{Results from the bootstrap procedure (for the power-law model) using the Moby Dick data set: \mbox{\texttt{bootstrap\_p(m\_pl)}}. The top row shows the mean estimate of parameters $\xmin$,  $\alpha$ and the $p$-value. The bottom row shows the estimate of standard deviation for each parameter. The dashed-lines give approximate 95\% confidence intervals.}\label{F1d}
\end{figure}

\subsection{Testing the power-law hypothesis}

Since it is possible to fit a power-law distribution to \textit{any} data set, it is appropriate to test whether it the observed data set actually follows a power-law. Clauset \textit{et al}, suggest that this hypothesis is tested using a goodness-of-fit test, via a bootstrapping procedure. This test generates a $p$-value that can be used to quantify the plausibility of the hypothesis. If the $p$-value is large, than any difference between the empirical data and the model can be explained with statistical fluctuations. If $p \simeq 0$, then the model does not provide a plausible fit to the data and another distribution may be more appropriate. In this scenario, 
\begin{align*}
&H_0: \mbox{data is generated from a power-law distribution.}\\
&H_1: \mbox{data is not generated from a power-law distribution.}
\end{align*}
\noindent To test these hypothesis, we use the \texttt{bootstrap\_p} function
<<eval=FALSE>>=
bs_p = bootstrap_p(m_pl)
@
\noindent The point estimate of the $p$-value is one of the elements of the \texttt{bs\_p} object\sidenote{Also given is the average time in seconds of a single bootstrap: \mbox{\texttt{bs\_p\$sim\_time} = \Sexpr{signif(bs_p$sim_time, 3)}}.}
<<>>=
bs_p$p
@
\noindent Alternatively we can plot the results

<<fig.keep='none', cache=TRUE>>=
plot(bs_p)
@

\noindent to obtain figure \ref{F1d}. The graph in the top right hand corner gives the cumulative estimate of the $p$-value; the final value of the purple line corresponds to \texttt{bs\_p\$p}. Also given are approximate 95\% confidence intervals. 



\subsection{Comparing distributions}

A second approach to assessing is a direct comparison of two models. A standard technique is
to use Vuong's test, which a likelihood ratio test for model selection using the
Kullback-Leibler criteria. The test statistic, $R$, is the ratio of the
log-likelihoods of the data between the two competing models. The sign of $R$
indicates which model is \textit{better}. Since the value of $R$ is obviously
subject to error, we use the method proposed by Vuong, 1989.\cite{Vuong1989}

To compare two distributions, each distribution must have the same lower threshold. So we first set the log normal distribution object to have the same $\xmin$ as the power law object:
<<cache=TRUE>>=
m_ln$setXmin(m_pl$getXmin())
@
\noindent Next we estimate the parameters for this particular value of $\xmin$:
<<cache=TRUE>>=
est = estimate_pars(m_ln)
m_ln$setPars(est)
@
\noindent Then we can compare distributions
<<cache=TRUE>>=
comp = compare_distributions(m_pl, m_ln)
@
\noindent This comparison gives a $p$-value of \Sexpr{comp[["p_two_sided"]]}. This $p$-value corresponds to the $p$-value on page 29 of the Clauset paper (the paper gives 0.69). The differences occur because we estimated the scaling parameter in the power law using the MLE, whereas Clauset et al, used a parameter scan, i.e.
<<cache=TRUE>>=
est = estimate_pars(m_pl, pars=seq(1.95, 1.96, 0.0005))
m_pl$setPars(est)
comp = compare_distributions(m_pl, m_ln)
@
\noindent now gives $p=\Sexpr{comp[["p_two_sided"]]}$.

Overall these results suggest that one model can't be favoured over the other. 


\subsection{Investigating the effect in $\xmin$}

The estimate of the scaling parameter, $\alpha$, is typically highly correlated with the threshold limit, $\xmin$. This relationship can be easily investigated with the \cc{poweRlaw} package. First, we create a vector of thresholds to scan
<<>>=
xmins = 1:1500
@

<<est_scan, echo=FALSE>>=
est_scan = 0*xmins
for(i in seq_along(xmins)){
  m_pl$setXmin(xmins[i])
  est_scan[i] = estimate_pars(m_pl)$pars
}
@

\begin{marginfigure}
\centering
<<echo=FALSE>>=
plot(xmins, est_scan, type="l", 
     panel.first=grid(), 
     xlab=expression(x[min]), ylab=expression(alpha), 
     ylim=c(1.6, 2.8), col=1)
abline(h=1.95, col=2, lty=2)
@
\caption{Estimated parameter values conditional on the threshold, $\xmin$. The horizontal line corresponds to $\alpha=1.95$.}\label{F1e}
\end{marginfigure}
\noindent then a vector to store the results
<<est_scan, echo=1, eval=FALSE>>=
@
\noindent Next, we loop over the $\xmin$ values and estimate the parameter value conditional on the $\xmin$ value:
<<est_scan, echo=2:4>>=
@
\noindent The results are plotted figure \ref{F1e}. For this data set, as the lower threshold increases, so does the point estimate of $\alpha$.




\clearpage

\section{Continuous data: electrical blackouts}

In this example, we will investigate the numbers of customers affected in electrical blackouts in the United States between 1984 and 2002.\cite{newman2005} The data set can be downloaded from Clauset's website:\sidenote{\url{http://goo.gl/BsqnP}} 
<<>>=
blackouts = read.table("blackouts.txt")
@
\noindent Although the \texttt{blackouts} data set is discrete, since the values are large, it makes sense to treat the data as continuous. Continuous power-law objects take vectors as inputs, so
<<cache=TRUE>>=
m_bl = conpl$new(blackouts$V1)
@
\noindent then we estimate the lower-bound via
<<cache=TRUE>>=
est = estimate_xmin(m_bl)
@
\noindent This gives a point estimate of $\xmin=\Sexpr{est$xmin}$. We can then update distribution object:
<<cache=TRUE>>=
m_bl$setXmin(est)
@
\noindent Using generic plot method
<<fig.keep='none', cache=TRUE>>=
plot(m_bl)
lines(m_bl, col=2, lwd=2)
@
\noindent we get figure \ref{F2a}. To fit a discrete log-normal distribution we follow a similar procedure:
<<m_bl_ln, echo=FALSE, cache=TRUE>>=
m_bl_ln = conlnorm$new(blackouts$V1)
est = estimate_xmin(m_bl_ln)
m_bl_ln$setXmin(est)
@


\begin{marginfigure}[5\baselineskip]
\centering
<<echo=FALSE, fig.width=4, fig.height=4>>=
plot(m_bl, pch=21, bg=1, 
     panel.first=grid(col="grey80"), 
     xlab="Blackouts", ylab="CDF")
lines(m_bl, col=2, lwd=3)
lines(m_bl_ln, col=3, lwd=3)
@

\caption{CDF plot of the blackout dataset with line of best fit. Since the minimum value of $x$ is large, we fit a continuous power-law as this is more it efficient. The power-law fit is the green line, the discrete log-normal is the red line.}\label{F2a}
\end{marginfigure}
<<m_bl_ln, eval=FALSE>>=
@
\noindent and add the line of best fit to the plot via
<<eval=FALSE>>=
lines(m_bl_ln, col=3, lwd=2)
@
\noindent It is clear from figure \ref{F2a} that the log-normal distribution provides a better fit to this data set.

\clearpage

\section{Multiple data sets: the American-Indian war}

In a recent paper, \citeauthor{Bohorquez2009} investigated insurgent attacks in Afghanistan, Iraq, Colombia, and Peru.\cite{Bohorquez2009} Each time, the data resembled power laws. \citeauthor{Friedman2013} used the power-law nature of casualties to infer under-reporting in the American-Indian war. Briefly, by fitting a power-law distribution to the observed process, the latent, unobserved casualties can be inferred.\cite{Friedman2013}  

The number of casualties observed in the American-Indian War can be obtained via
<<>>=
data("native_american")
data("us_american")
@
\noindent Each data set is a data frame with two columns. The first column is number of casualties recorded, the second the conflict date
<<>>=
head(native_american, 3)
@
\noindent The records span around one hundred years, 1776 -- 1890. The data is plotted in figure \ref{F3a}. 
\begin{figure}[!b]
\centering
<<echo=FALSE, fig.width=6, fig.height=4>>=
plot(native_american$Date, native_american$Cas, 
     log="y", pch=21, bg=1, 
     ylim=c(1, 2000), 
     cex=0.5, panel.first=grid(col="grey70"), 
     xlab="Date", ylab="#Casualties")

points(us_american$Date, us_american$Cas, 
       pch=24, bg=2, cex=0.5)
@
\caption{Casualty record for the Indian-American war, 1776 -- 1890. Native Americans casualties (purple circles) and US Americans casualties (green triangles). Data taken from \citeauthor{Friedman2013}.}\label{F3a}
\end{figure}

It is straightforward to fit a discrete power-law to this data set. First, we create discrete power-law objects:
<<cache=TRUE>>=
m_na = displ$new(native_american$Cas)
m_us = displ$new(us_american$Cas)
@
\noindent then we estimate $\xmin$ for each data set:
<<cache=TRUE>>=
est_na = estimate_xmin(m_na, pars=seq(1.5, 2.5, 0.001))
est_us = estimate_xmin(m_us, pars=seq(1.5, 2.5, 0.001))
@
\noindent and update the power-law objects
<<cache=TRUE>>=
m_na$setXmin(est_na)
m_us$setXmin(est_us)
@
\noindent The resulting fitted distributions can be plotted on the same figure
<<fig.keep='none', cache=TRUE, tidy=FALSE>>=
plot(m_na)
lines(m_na)
##Don't create a new plot
##Just store the output
d = plot(m_us, draw=FALSE)
points(d$x, d$y, col=2)
lines(m_us, col=2)
@
\begin{marginfigure}
<<echo=FALSE, fig.width=4, fig.height=4>>=
plot(m_na, bg=1, pch=21, cex=0.5,  
     panel.first=grid(col="grey70"), 
     xlab="#Casualties", ylab="CDF")
lines(m_na, lwd=2, col=1)

d = plot(m_us, draw=FALSE)
points(d$x, d$y, bg=2, pch=24, cex=0.5)
lines(m_us, lwd=2, col=2)
@
\caption{Plots of the CDFs for the Native American and US American casualties. The lines of best fit are also given.}\label{F3b}
\end{marginfigure}
\noindent The result is given in figure \ref{F3b}. The tails of the distributions appear to follow a power-law. This is consistent with the expectation that smaller-scale engagements are less likely to be recorded. However, for larger scale engagements, it is very likely that a record is made.


\clearpage

\bibliography{poweRlaw}
\bibliographystyle{plainnat}

\clearpage

\section*{Session Info}

<<>>=
print(sessionInfo(), locale = FALSE)
@








\end{document}